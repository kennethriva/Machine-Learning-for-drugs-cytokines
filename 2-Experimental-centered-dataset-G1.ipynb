{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental-centered dataset\n",
    "\n",
    "One special type of normalization is centering feature values using averages of the feature in different conditions. Some papers are describing this normalization as Moving Average (MA) calculation. Thus, the experimental normalization could be defined as:\n",
    "\n",
    "*Experimental Normalized feature = Feature - Feature Avg using an experimental condition*\n",
    "\n",
    "Several files will be created:\n",
    "- details including original dataset, experimental-averages for features, experimental-centered features\n",
    "- only the experimental-centered features + output variable (non-standardized dataset for ML)\n",
    "\n",
    "Different scalers will be used in other scripts using only training set and applying the same transformation to the test / validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import pandas for dataframe handling, os for working with files and our *ds_utils.py* with some functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ds_utils # our set of useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose working folder, dataset file to process, files to save for the details and only the centered features, names for the normalized and standardized dataset files, names of experimental condition columns, number of the columns for the features (starting and ending columns), output variable name. The normalized features will be added to the initial dataset generating a file with details. At the end, only the centered features and the output variable will be save too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working folder\n",
    "WorkingFolder = './datasets/'\n",
    "\n",
    "# dataset file to be pre-processed\n",
    "dsOriginFile  = 'ds_raw.csv'\n",
    "ds_G1         = 'ds.G1_raw.csv'\n",
    "\n",
    "# resulting files\n",
    "dsDetailsFile  = 'ds_details.csv'        # original dataset + centered features using conditions\n",
    "dsOnlyMAsFile  = 'ds_MA.csv' # only the centered features using conditions\n",
    "\n",
    "dsDetailsFile_G1  = 'ds.G1_details.csv'        # original dataset + centered features using conditions\n",
    "dsOnlyMAsFile_G1  = 'ds.G1_MA.csv' # only the centered features using conditions\n",
    "\n",
    "# experimental condition columns to use to center features (\"MAs\")\n",
    "ExperimCondList = ['STANDARD_TYPE_UNITSj','ASSAY_CHEMBLID',\n",
    "                   'ASSAY_TYPE','ASSAY_ORGANISM',\n",
    "                   'ORGANISM','TARGET_CHEMBLID']\n",
    "\n",
    "# output variable name\n",
    "outputVar = 'Lij'\n",
    "\n",
    "# starting and ending columns for features in the original dataset\n",
    "startColFeatures = 19\n",
    "endColFeatures   = 271"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the original file to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Reading original dataset ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('-> Reading original dataset ...')\n",
    "df_raw = pd.read_csv(os.path.join(WorkingFolder, dsOriginFile))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Reading G1 raw dataset ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('-> Reading G1 raw dataset ...')\n",
    "df_G1_raw = pd.read_csv(os.path.join(WorkingFolder, ds_G1))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the original dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center each feature using the experimental condition columns:\n",
    "- create a temporal dataframe\n",
    "- copy the experimental columns and the features to center\n",
    "- for each feature, center the values as difference between the feature value and the average of this feature for a specific value of a specific experimental condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Center features using experimental conditions ...\n",
      "--> Reading the experimental data ...\n",
      "Index(['STANDARD_TYPE_UNITSj', 'ASSAY_CHEMBLID', 'ASSAY_TYPE',\n",
      "       'ASSAY_ORGANISM', 'ORGANISM', 'TARGET_CHEMBLID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# copy only data for the experimental condition columns into a temporal dataframe\n",
    "print('-> Center features using experimental conditions ...')\n",
    "print('--> Reading the experimental data ...')\n",
    "newdf = df_raw[ExperimCondList].copy()\n",
    "\n",
    "# get the experimental condition names from the new dataframe\n",
    "exper_conds = newdf.columns\n",
    "print(exper_conds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Reading the feature names ...\n",
      "Index(['ALogP', 'ALogp2', 'AMR', 'apol', 'naAromAtom', 'nAromBond', 'nAtom',\n",
      "       'ATSc1', 'ATSc2', 'ATSc3',\n",
      "       ...\n",
      "       'comp_L', 'comp_K', 'comp_M', 'comp_F', 'comp_P', 'comp_S', 'comp_T',\n",
      "       'comp_W', 'comp_Y', 'comp_V'],\n",
      "      dtype='object', length=253)\n"
     ]
    }
   ],
   "source": [
    "# get list of descriptor names\n",
    "print('--> Reading the feature names ...')\n",
    "descriptors = df_raw[df_raw.columns[startColFeatures-1:endColFeatures]].columns\n",
    "print(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Reading the G1 feature names ...\n",
      "Index(['ALogP', 'ALogp2', 'AMR', 'apol', 'naAromAtom', 'nAromBond', 'nAtom',\n",
      "       'ATSc1', 'ATSc2', 'ATSc3',\n",
      "       ...\n",
      "       'comp_L', 'comp_K', 'comp_M', 'comp_F', 'comp_P', 'comp_S', 'comp_T',\n",
      "       'comp_W', 'comp_Y', 'comp_V'],\n",
      "      dtype='object', length=253)\n"
     ]
    }
   ],
   "source": [
    "# get list of descriptor names G1\n",
    "print('--> Reading the G1 feature names ...')\n",
    "descriptors_G1 = df_G1_raw[df_G1_raw.columns[startColFeatures-1:endColFeatures]].columns\n",
    "print(descriptors_G1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the centered features using the averages of theses features for experimental condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Centering features using 1518 pairs of experiment - feature ...\n",
      "Done!\n",
      "Columns of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['CMPD_CHEMBLID', 'CANONICAL_SMILES', 'PROTEIN_ACCESSION', 'ACTIVITY_ID',\n",
       "       'STANDARD_TYPE_UNITSj', 'STANDARD_VALUE', 'ASSAY_CHEMBLID',\n",
       "       'ASSAY_TYPE', 'ASSAY_ORGANISM', 'CURATED_BY',\n",
       "       ...\n",
       "       'avg-comp_S-TARGET_CHEMBLID', 'MA-comp_S-TARGET_CHEMBLID',\n",
       "       'avg-comp_T-TARGET_CHEMBLID', 'MA-comp_T-TARGET_CHEMBLID',\n",
       "       'avg-comp_W-TARGET_CHEMBLID', 'MA-comp_W-TARGET_CHEMBLID',\n",
       "       'avg-comp_Y-TARGET_CHEMBLID', 'MA-comp_Y-TARGET_CHEMBLID',\n",
       "       'avg-comp_V-TARGET_CHEMBLID', 'MA-comp_V-TARGET_CHEMBLID'],\n",
       "      dtype='object', length=3307)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list only for the MA names (centered values using experimental conditions)\n",
    "onlyMAs = []\n",
    "\n",
    "# FOR each pair Exp cond - feature calculate MA and add it to the original dataset\n",
    "print('--> Centering features using', len(descriptors)*len(exper_conds),\n",
    "      'pairs of experiment - feature ...')\n",
    "\n",
    "for experim in exper_conds:    # for each experimental condition colunm\n",
    "    for descr in descriptors:  # for each feature\n",
    "        \n",
    "        # calculate the Avg for a pair of experimental conditions and a descriptor\n",
    "        avgs = df_raw.groupby(experim, as_index = False).agg({descr:\"mean\"})\n",
    "        \n",
    "        # rename the avg column name\n",
    "        avgs = avgs.rename(columns={descr: 'avg-'+ descr + '-' + experim})\n",
    "        \n",
    "        # merge an Avg to datasets\n",
    "        df_raw = pd.merge(df_raw, avgs, on=[experim])\n",
    "        df_G1_raw = pd.merge(df_G1_raw, avgs, on=[experim])\n",
    "        \n",
    "        \n",
    "        # add MA to the datasets for pair Exp cond - descr\n",
    "        df_raw['MA-'+descr+'-'+experim] = df_raw[descr] - df_raw['avg-'+descr+'-'+experim]\n",
    "        df_G1_raw['MA-'+descr+'-'+experim] = df_G1_raw[descr] - df_G1_raw['avg-'+descr+'-'+experim]\n",
    "        \n",
    "        # add the name of the MA to the list\n",
    "        onlyMAs.append('MA-'+descr+'-'+experim)\n",
    "        \n",
    "print(\"Done!\")\n",
    "# print the new column names\n",
    "print('Columns of the dataset:')\n",
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all the details as file: ds_raw with Avgs, MAs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Saving the dataset with all details ...\n",
      "No of centered features using experiments: 1518\n",
      "Done!\n",
      "--> Saving the G1 dataset with all details ...\n",
      "No of centered features using experiments: 1518\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('--> Saving the dataset with all details ...')\n",
    "df_raw.to_csv(os.path.join(WorkingFolder, dsDetailsFile), index=False)\n",
    "# print only the names of columns for MAs\n",
    "print('No of centered features using experiments:', len(onlyMAs))\n",
    "print('Done!')\n",
    "\n",
    "print('--> Saving the G1 dataset with all details ...')\n",
    "df_G1_raw.to_csv(os.path.join(WorkingFolder, dsDetailsFile_G1), index=False)\n",
    "# print only the names of columns for MAs\n",
    "print('No of centered features using experiments:', len(onlyMAs))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the file with the original dataset is containing extra columns with the averages and centered values.\n",
    "\n",
    "In the next step, you will save only the centered features using experiments as dataset for future Machine Learning calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of removed rows due duplication: -1585\n",
      "-> Saving non-standardized dataset with MAs ...\n",
      "Done!\n",
      "-> Saving non-standardized dataset with MAs G1 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# get only the MAs + output variable and save it as the final dataset for ML\n",
    "# add ouput name to the list of MAs for the final ds\n",
    "onlyMAs.append(outputVar)\n",
    "\n",
    "# get only the MAs + output var as a new dataframe\n",
    "df_MA =df_raw[onlyMAs].copy()\n",
    "df_MA_G1 =df_G1_raw[onlyMAs].copy()\n",
    "\n",
    "# no of rows before removing duplications\n",
    "dsIniLength = len(df_MA)\n",
    "dsIniLength_G1 = len(df_MA_G1)\n",
    "\n",
    "# remove duplicated rows!\n",
    "df_MA.drop_duplicates(inplace=True)\n",
    "# df_MA_G1.drop_duplicates(inplace=True)\n",
    "\n",
    "# check the number of removed cases!\n",
    "print('No of removed rows due duplication:', len(df_MA) - dsIniLength)\n",
    "#print('No of removed rows due duplication G1:', len(df_MA_G1) - dsIniLength_G1)\n",
    "\n",
    "# save ds with only MAs + output variable for ML\n",
    "print('-> Saving non-standardized dataset with MAs ...')\n",
    "df_MA.to_csv(os.path.join(WorkingFolder, dsOnlyMAsFile), index=False)\n",
    "print('Done!')\n",
    "\n",
    "print('-> Saving non-standardized dataset with MAs G1 ...')\n",
    "df_MA_G1.to_csv(os.path.join(WorkingFolder, dsOnlyMAsFile_G1), index=False)\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental-centered features dataset: rows = 12906 columns = 1519\n",
      "Experimental-centered features dataset G1: rows = 1384 columns = 1519\n"
     ]
    }
   ],
   "source": [
    "# print the dimension of the final MA dataset\n",
    "print('Experimental-centered features dataset: rows =', len(df_MA), 'columns =', len(df_MA.columns))\n",
    "print('Experimental-centered features dataset G1: rows =', len(df_MA_G1), 'columns =', len(df_MA_G1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check repeated column names\n",
    "l = list(df_MA.columns)\n",
    "set([x for x in l if l.count(x) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check repeated column names\n",
    "l = list(df_MA_G1.columns)\n",
    "set([x for x in l if l.count(x) > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other transformation such as scalling will be based only on the training subset and apply to the test/validation subsets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
