{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Baseline Generator\n",
    "\n",
    "This is the first script to use a set of ML method with default parameters in order to obtain baseline results. We will use the following *sklearn* classifiers:\n",
    "\n",
    "1. KNeighborsClassifier - Nearest Neighbors\n",
    "2. LinearSVC - Linear Support vector machine (SVM)\n",
    "3. LogisticRegression - Logistic regression\n",
    "4. SVC - Support vector machine (SVM) with Radial Basis Functions (RBF)\n",
    "5. AdaBoostClassifier - AdaBoost\n",
    "6. GaussianNB - Gaussian Naive Bayes\n",
    "7. MLPClassifier - Multi-Layer Perceptron (MLP) / Neural Networks\n",
    "8. DecisionTreeClassifier - Decision Trees\n",
    "9. RandomForestClassifier - Random Forest\n",
    "10. GradientBoostingClassifier - Gradient Boosting\n",
    "11. BaggingClassifier - ensemble meta-estimator Bagging\n",
    "12. XGBClassifier - XGBoost\n",
    "\n",
    "*Note: More advanced hyperparameter search will be done in future scripts!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scripts\n",
    "from ds_utils import *\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will show the chosen procedure for the Machine Learning baseline generator.\n",
    "The first step is to create a list of train and test datasets which will be used to generate and estimae a set of performances of more common used algorithms. In order to have a wide approximation several metrics will be used for every model.\n",
    "\n",
    "### Step 1 - List of datasets and classifiers\n",
    "So as a first step lets define a list o datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Files to use for ML baseline generator:\n",
      "Training sets:\n",
      " ['fs.rf.m.ds_MA_tr.csv', 'fs.rf.s.ds_MA_tr.csv', 'm.ds_MA_tr.csv', 'pca0.99.m.ds_MA_tr.csv', 'pca0.99.o.ds_MA_tr.csv', 'pca0.99.s.ds_MA_tr.csv', 's.ds_MA_tr.csv']\n",
      "Test sets:\n",
      " ['fs.rf.m.ds_MA_ts.csv', 'fs.rf.s.ds_MA_ts.csv', 'm.ds_MA_ts.csv', 'pca0.99.m.ds_MA_ts.csv', 'pca0.99.o.ds_MA_ts.csv', 'pca0.99.s.ds_MA_ts.csv', 's.ds_MA_ts.csv']\n"
     ]
    }
   ],
   "source": [
    "# dataset folder\n",
    "WorkingFolder  = './datasets/'\n",
    "# BasicMLResults = 'ML_basic.csv' # a file with all the statistis for ML models\n",
    "\n",
    "# Split details\n",
    "seed = 0          # for reproductibility\n",
    "\n",
    "# output variable\n",
    "outVar = 'Lij'    \n",
    "\n",
    "# parameter for ballanced (equal number of examples in all classes) and non-ballanced dataset \n",
    "class_weight = \"balanced\" # use None for ballanced datasets!\n",
    "\n",
    "\n",
    "# set list of train and test files\n",
    "\n",
    "listFiles_tr = [col for col in os.listdir(WorkingFolder) if ('tr' in col)\n",
    "                and (col[:5] != 'o.ds_') ]\n",
    "\n",
    "listFiles_ts = [col for col in os.listdir(WorkingFolder) if ('ts' in col)\n",
    "                and (col[:5] != 'o.ds_') ]\n",
    "\n",
    "# Check the list of files to process\n",
    "print('* Files to use for ML baseline generator:')\n",
    "print('Training sets:\\n', listFiles_tr)\n",
    "print('Test sets:\\n',listFiles_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined our list of datasets, let's call baseline_generator function which will generate a dataframe with all performances for every combination of dataset and algorithm. Remenber we are using a set of algorithms as a baseline where are included basic, complex and ensemble methods. For more information you can call baseline_classifiers method fror the ds_utils.py script to see which algorithms and parameters are being used for the baseline. Another aspect to point out is that weights for algorithms that used this parameter are calculated using set_weights method based on the class_weight.compute_class_weight sklearn method. More information can be found at: https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html.\n",
    "\n",
    "Let's verify the ballance of the classes and create the classifier definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights {0: 0.6792961543919398, 1: 1.894341115947764}\n",
      "**************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       " LinearSVC(C=1.0, class_weight={0: 0.6792961543919398, 1: 1.894341115947764},\n",
       "      dual=True, fit_intercept=True, intercept_scaling=1,\n",
       "      loss='squared_hinge', max_iter=5000, multi_class='ovr', penalty='l2',\n",
       "      random_state=0, tol=0.0001, verbose=0),\n",
       " LogisticRegression(C=1.0,\n",
       "           class_weight={0: 0.6792961543919398, 1: 1.894341115947764},\n",
       "           dual=False, fit_intercept=True, intercept_scaling=1,\n",
       "           max_iter=500, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "           random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "           warm_start=False),\n",
       " SVC(C=1.0, cache_size=200,\n",
       "   class_weight={0: 0.6792961543919398, 1: 1.894341115947764}, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
       "   verbose=False),\n",
       " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=0),\n",
       " GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_sizes=(20,), learning_rate='constant',\n",
       "        learning_rate_init=0.001, max_iter=1500, momentum=0.9,\n",
       "        n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "        random_state=0, shuffle=False, solver='adam', tol=0.0001,\n",
       "        validation_fraction=0.1, verbose=False, warm_start=False),\n",
       " DecisionTreeClassifier(class_weight={0: 0.6792961543919398, 1: 1.894341115947764},\n",
       "             criterion='gini', max_depth=None, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=0, splitter='best'),\n",
       " RandomForestClassifier(bootstrap=True,\n",
       "             class_weight={0: 0.6792961543919398, 1: 1.894341115947764},\n",
       "             criterion='gini', max_depth=None, max_features='auto',\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, n_jobs=-1, oob_score=False, random_state=0,\n",
       "             verbose=0, warm_start=False),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               n_iter_no_change=None, presort='auto', random_state=0,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False),\n",
       " BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "          bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "          n_estimators=10, n_jobs=None, oob_score=False, random_state=0,\n",
       "          verbose=0, warm_start=False),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bytree=0.6, gamma=0.1, learning_rate=0.1,\n",
       "        max_delta_step=0, max_depth=6, min_child_weight=1, missing=None,\n",
       "        n_estimators=1000, n_jobs=-1, nthread=None,\n",
       "        objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "        reg_lambda=1, scale_pos_weight=0.35859230878387965, seed=0,\n",
       "        silent=True, subsample=0.6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling baseline_classifiers to see which algorithms and parameters are being used. Remember that\n",
    "# baseline_classifiers() need a y_tr_data argument to be executed, it can be any y_tr_data if you just want\n",
    "# to see the output\n",
    "\n",
    "y_tr_data = datasets_parser(listFiles_tr[0], listFiles_ts[0],outVar=outVar, WorkingFolder=WorkingFolder)[1]\n",
    "ML_methods_used = baseline_classifiers(y_tr_data)\n",
    "ML_methods_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Baseline generator for all datasets and ML methods\n",
    "Once settle this. The next step is to call the baseline_generator method which takes as arguments a list of train and test sets we define previously. This function will calculate some metrics for each combination of train-test sets and will create a dataframe with all the performances. The final dataframe is sorted by AUC value, so the first row will correspond to the algorithm and dataset which achieve better perforamce. \n",
    "\n",
    "For each dataset and method we will print here only the test **AUC** and **Accuracy**. The rest of statistics will be save on a local CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Generating Basic Machine Learning baseline...\n",
      "-> Dataset: ./datasets/ fs.rf.m.ds_MA_tr.csv ...\n",
      "class weights {0: 0.6792961543919398, 1: 1.894341115947764}\n",
      "**************************************\n",
      "* Classifier: KNeighborsClassifier ...\n",
      "[0.7100568273922878, 0.7904]\n",
      "* Classifier: LinearSVC ...\n",
      "[0.7565345441158424, 0.7575]\n",
      "* Classifier: LogisticRegression ...\n",
      "[0.7292373979084006, 0.7901]\n",
      "* Classifier: SVC ...\n",
      "[0.7206875341820325, 0.7954]\n",
      "* Classifier: AdaBoostClassifier ...\n",
      "[0.7103583110250595, 0.7998]\n",
      "* Classifier: GaussianNB ...\n",
      "[0.6859775265919168, 0.7415]\n",
      "* Classifier: MLPClassifier ...\n",
      "[0.6754528693212376, 0.7904]\n",
      "* Classifier: DecisionTreeClassifier ...\n",
      "[0.6930495145204516, 0.7447]\n",
      "* Classifier: RandomForestClassifier ...\n",
      "[0.729241942887789, 0.7744]\n",
      "* Classifier: GradientBoostingClassifier ...\n",
      "[0.7032529932476757, 0.8067]\n",
      "* Classifier: BaggingClassifier ...\n",
      "[0.6832755363454427, 0.7751]\n",
      "* Classifier: XGBClassifier ...\n",
      "[0.6562912362192438, 0.7779]\n",
      "-> Dataset: ./datasets/ fs.rf.s.ds_MA_tr.csv ...\n",
      "class weights {0: 0.6792961543919398, 1: 1.894341115947764}\n",
      "**************************************\n",
      "* Classifier: KNeighborsClassifier ...\n",
      "[0.6774185729067718, 0.7782]\n",
      "* Classifier: LinearSVC ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7659502264157232, 0.7563]\n",
      "* Classifier: LogisticRegression ...\n",
      "[0.7597584494954315, 0.76]\n",
      "* Classifier: SVC ...\n",
      "[0.7333437867859269, 0.7917]\n",
      "* Classifier: AdaBoostClassifier ...\n",
      "[0.7081532385250634, 0.7982]\n",
      "* Classifier: GaussianNB ...\n",
      "[0.6751006334186274, 0.7462]\n",
      "* Classifier: MLPClassifier ...\n",
      "[0.6882454713067877, 0.7885]\n",
      "* Classifier: DecisionTreeClassifier ...\n",
      "[0.7425678224549251, 0.7694]\n",
      "* Classifier: RandomForestClassifier ...\n",
      "[0.7481164847917414, 0.7804]\n",
      "* Classifier: GradientBoostingClassifier ...\n",
      "[0.7000601452272415, 0.802]\n",
      "* Classifier: BaggingClassifier ...\n",
      "[0.707380592029015, 0.7876]\n",
      "* Classifier: XGBClassifier ...\n",
      "[0.6478943867989558, 0.7801]\n",
      "-> Dataset: ./datasets/ m.ds_MA_tr.csv ...\n",
      "class weights {0: 0.6792961543919398, 1: 1.894341115947764}\n",
      "**************************************\n",
      "* Classifier: KNeighborsClassifier ...\n",
      "[0.6923829175434689, 0.7879]\n",
      "* Classifier: LinearSVC ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7996459461056344, 0.7935]\n",
      "* Classifier: LogisticRegression ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.784102874093466, 0.7863]\n",
      "* Classifier: SVC ...\n",
      "[0.7360798643778151, 0.7979]\n",
      "* Classifier: AdaBoostClassifier ...\n",
      "[0.7133716323596472, 0.7942]\n",
      "* Classifier: GaussianNB ...\n",
      "[0.6555572220479979, 0.6341]\n",
      "* Classifier: MLPClassifier ...\n",
      "[0.7292972401370159, 0.7935]\n",
      "* Classifier: DecisionTreeClassifier ...\n",
      "[0.6866426085757702, 0.756]\n",
      "* Classifier: RandomForestClassifier ...\n",
      "[0.7078373624575612, 0.786]\n",
      "* Classifier: GradientBoostingClassifier ...\n",
      "[0.7222661570229779, 0.8067]\n",
      "* Classifier: BaggingClassifier ...\n",
      "[0.7063928165085771, 0.7845]\n",
      "* Classifier: XGBClassifier ...\n",
      "[0.6877000737801655, 0.781]\n",
      "-> Dataset: ./datasets/ pca0.99.m.ds_MA_tr.csv ...\n",
      "class weights {0: 0.6792961543919398, 1: 1.894341115947764}\n",
      "**************************************\n",
      "* Classifier: KNeighborsClassifier ...\n",
      "[0.6947553967842757, 0.7892]\n",
      "* Classifier: LinearSVC ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7522562035181171, 0.7719]\n",
      "* Classifier: LogisticRegression ...\n",
      "[0.7522562035181171, 0.7719]\n",
      "* Classifier: SVC ...\n"
     ]
    }
   ],
   "source": [
    "baseline = baseline_generator(listFiles_tr, listFiles_ts, outVar, WorkingFolder)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the previous result it seems that the minitrain.csv dataset tend to get better performances that s.ds_MA.tr.csv. On the other hand Gradient Boosting Classifier is the method that achieves better performance, so is probably a good candidate for the minitrain.csv dataset. We could try some combination of parameters on that dataset and algorithm in the gridsearch strategy. But before we go any further we can plot the ROC curves for this baseline so that we can have a graphic comparaison across the methods used for the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - AUROCs for the best dataset\n",
    "Plot the roc curves for the baseline from above. We can do this by calling the ROC_baseline_plot which will use an unique pair of train a test datasets. Since we can conclude that the dataset with better performances is the minitrain.csv we can use it for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test sets used for ROC_baseline_plot\n",
    "newFile_tr = 'minitrain.csv' # new training data \n",
    "newFile_ts = 'minitest.csv' # new testing data\n",
    "roc_curve_baseline = ROC_baseline_plot(newFile_tr, newFile_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous plot we can confirm once again that GradientBoostingClassifier is a good candidate to optimize for this dataset.\n",
    "\n",
    "In another notebook we will analyze how to look for a good combination of parameters for a set of chosen algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
